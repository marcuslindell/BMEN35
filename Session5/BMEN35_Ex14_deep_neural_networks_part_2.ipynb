{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35/blob/main/Session5/BMEN35_Ex14_deep_neural_networks_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SZj5dXAsym6"
   },
   "source": [
    "# Deep Neural Networks part 2\n",
    " \n",
    "In this notebook  we will use the models we generated and saved in the previous part and see how they can be used. Let's start with importing our usual suspects and few new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QDSm7FrGsv_u"
   },
   "outputs": [],
   "source": [
    "# The usual imports\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import argmax\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import time\n",
    "import numpy as np\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGaffjr2vavs"
   },
   "source": [
    "First we will upload our model files. Choose all the files that you saved and downloaded from the previuos notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yafK1sAtvlh_"
   },
   "outputs": [],
   "source": [
    "# _ = files.upload() # Select all the model files (*.h5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3W84BOh3-dU"
   },
   "source": [
    "We will load our dataset again, cast and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dBHVZUODwTIO"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# We will use one hot encoding for the target/classes values\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "# Cast to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize to range 0-1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwJ_0Xxy4Naa"
   },
   "source": [
    "We will use our previously trained models now. We will load our models and see how they perform on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cnA0O218EjKD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 13:07:32.223179: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Model A is 0.9633 \n",
      "Test accuracy for Model B is 0.9713 \n",
      "Test accuracy for Model C is 0.9797 \n",
      "Test accuracy for Model D is 0.9603 \n"
     ]
    }
   ],
   "source": [
    "model_a = load_model('model_a.h5')\n",
    "test_results = model_a.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy for Model A is {:.4f} \".format(test_results[1]))\n",
    "model_b = load_model('model_b.h5')\n",
    "test_results = model_b.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy for Model B is {:.4f} \".format(test_results[1]))\n",
    "model_c = load_model('model_c.h5')\n",
    "test_results = model_c.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy for Model C is {:.4f} \".format(test_results[1]))\n",
    "model_d = load_model('model_d.h5')\n",
    "test_results = model_d.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy for Model D is {:.4f} \".format(test_results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2YC_JCcFgde"
   },
   "source": [
    "Next we will check what can be done using the latter models (model_e and model_f). First we need to reshape the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oVN5JG8rwzlR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Model E is 0.9055 \n",
      "Test accuracy for Model F is 0.9357 \n"
     ]
    }
   ],
   "source": [
    "num_pixels = 784\n",
    "X_test_r = X_test.reshape((X_test.shape[0], num_pixels))\n",
    "\n",
    "model_e = load_model('model_e.h5')\n",
    "test_results = model_e.evaluate(X_test_r, y_test, verbose=0)\n",
    "print(\"Test accuracy for Model E is {:.4f} \".format(test_results[1]))\n",
    "model_f = load_model('model_f.h5')\n",
    "test_results = model_f.evaluate(X_test_r, y_test, verbose=0)\n",
    "print(\"Test accuracy for Model F is {:.4f} \".format(test_results[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQnPGo1p4jD-"
   },
   "source": [
    "Interestingly, the plain vanilla neural network performed quite well.\n",
    "\n",
    "Next we will see how our trained models perform on a sample image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7sp6ZK5xtnC"
   },
   "outputs": [],
   "source": [
    "# img = files.upload() # Select sample_image1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ArRcrbqnyDU4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = load_img('sample_image1.png', color_mode = \"grayscale\", target_size=(28, 28))\n",
    "#img = np.asarray(Image.open(filename))\n",
    "# plot the sample\n",
    "print(img.size)\n",
    "fig = plt.figure\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "# convert to array\n",
    "img = img_to_array(img)\n",
    "# reshape into a single sample with 1 channel\n",
    "img = img.reshape(1, 28, 28, 1)\n",
    "# prepare pixel data\n",
    "img = img.astype('float32')\n",
    "img = img / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9a2NnmIyY-O"
   },
   "source": [
    "Now lets use the uploaded image as input to our models, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q_HlZZPiyeNy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step\n",
      "Model A predicts 3 \n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Model B predicts 8 \n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Model C predicts 5 \n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Model D predicts 7 \n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe2b2a4d940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Model F predicts 2 \n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe28967be50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Model E predicts 3 \n"
     ]
    }
   ],
   "source": [
    "y_hata = model_a.predict(img)\n",
    "print(\"Model A predicts {:d} \".format(argmax(y_hata)))\n",
    "y_hatb = model_b.predict(img)\n",
    "print(\"Model B predicts {:d} \".format(argmax(y_hatb)))\n",
    "y_hatc = model_c.predict(img)\n",
    "print(\"Model C predicts {:d} \".format(argmax(y_hatc)))\n",
    "y_hatd = model_d.predict(img)\n",
    "print(\"Model D predicts {:d} \".format(argmax(y_hatd)))\n",
    "# For the latter models, we will reshape the image\n",
    "img_r = img.reshape(1,784)\n",
    "y_hate = model_e.predict(img_r)\n",
    "print(\"Model F predicts {:d} \".format(argmax(y_hate)))\n",
    "y_hatf = model_f.predict(img_r)\n",
    "print(\"Model E predicts {:d} \".format(argmax(y_hatf)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AWvag7GxuK9"
   },
   "source": [
    "Interestingly, all models are not able to classify this image correctly.\n",
    "## The End"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/zc3qXERlCXMX9yFpgiU5",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
