{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/zc3qXERlCXMX9yFpgiU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35/blob/main/Session5/BMEN35_Ex14_deep_neural_networks_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks part 2\n",
        " \n",
        "In this notebook  we will use the models we generated and saved in the previous part and see how they can be used. Let's start with importing our usual suspects and few new ones."
      ],
      "metadata": {
        "id": "-SZj5dXAsym6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDSm7FrGsv_u"
      },
      "outputs": [],
      "source": [
        "# The usual imports\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import argmax\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import time\n",
        "import numpy as np\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will upload our model files. Choose all the files that you saved and downloaded from the previuos notebook"
      ],
      "metadata": {
        "id": "tGaffjr2vavs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = files.upload() # Select all the model files (*.h5)\n"
      ],
      "metadata": {
        "id": "yafK1sAtvlh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load our dataset again, cast and scale the data."
      ],
      "metadata": {
        "id": "b3W84BOh3-dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# We will use one hot encoding for the target/classes values\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "# Cast to float\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# Normalize to range 0-1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "metadata": {
        "id": "dBHVZUODwTIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use our previously trained models now. We will load our models and see how they perform on the test set."
      ],
      "metadata": {
        "id": "rwJ_0Xxy4Naa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_a = load_model('model_a.h5')\n",
        "test_results = model_a.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test accuracy for Model A is {:.4f} \".format(test_results[1]))\n",
        "model_b = load_model('model_b.h5')\n",
        "test_results = model_b.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test accuracy for Model B is {:.4f} \".format(test_results[1]))\n",
        "model_c = load_model('model_c.h5')\n",
        "test_results = model_c.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test accuracy for Model C is {:.4f} \".format(test_results[1]))\n",
        "model_d = load_model('model_d.h5')\n",
        "test_results = model_d.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test accuracy for Model D is {:.4f} \".format(test_results[1]))"
      ],
      "metadata": {
        "id": "cnA0O218EjKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will check what can be done using the latter models (model_e and model_f). First we need to reshape the input data "
      ],
      "metadata": {
        "id": "o2YC_JCcFgde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_pixels = 784\n",
        "X_test_r = X_test.reshape((X_test.shape[0], num_pixels))\n",
        "\n",
        "model_e = load_model('model_e.h5')\n",
        "test_results = model_e.evaluate(X_test_r, y_test, verbose=0)\n",
        "print(\"Test accuracy for Model E is {:.4f} \".format(test_results[1]))\n",
        "model_f = load_model('model_f.h5')\n",
        "test_results = model_f.evaluate(X_test_r, y_test, verbose=0)\n",
        "print(\"Test accuracy for Model F is {:.4f} \".format(test_results[1]))\n"
      ],
      "metadata": {
        "id": "oVN5JG8rwzlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly, the plain vanilla neural network performed quite well.\n",
        "\n",
        "Next we will see how our trained models perform on a sample image. "
      ],
      "metadata": {
        "id": "tQnPGo1p4jD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = files.upload() # Select sample_image1.png"
      ],
      "metadata": {
        "id": "M7sp6ZK5xtnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = load_img('sample_image1.png', color_mode = \"grayscale\", target_size=(28, 28))\n",
        "#img = np.asarray(Image.open(filename))\n",
        "# plot the sample\n",
        "print(img.size)\n",
        "fig = plt.figure\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "# convert to array\n",
        "img = img_to_array(img)\n",
        "# reshape into a single sample with 1 channel\n",
        "img = img.reshape(1, 28, 28, 1)\n",
        "# prepare pixel data\n",
        "img = img.astype('float32')\n",
        "img = img / 255.0"
      ],
      "metadata": {
        "id": "ArRcrbqnyDU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets use the uploaded image as input to our models, "
      ],
      "metadata": {
        "id": "c9a2NnmIyY-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hata = model_a.predict(img)\n",
        "print(\"Model A predicts {:d} \".format(argmax(y_hata)))\n",
        "y_hatb = model_b.predict(img)\n",
        "print(\"Model B predicts {:d} \".format(argmax(y_hatb)))\n",
        "y_hatc = model_c.predict(img)\n",
        "print(\"Model C predicts {:d} \".format(argmax(y_hatc)))\n",
        "y_hatd = model_d.predict(img)\n",
        "print(\"Model D predicts {:d} \".format(argmax(y_hatd)))\n",
        "# For the latter models, we will reshape the image\n",
        "img_r = img.reshape(1,784)\n",
        "y_hate = model_e.predict(img_r)\n",
        "print(\"Model F predicts {:d} \".format(argmax(y_hate)))\n",
        "y_hatf = model_f.predict(img_r)\n",
        "print(\"Model E predicts {:d} \".format(argmax(y_hatf)))\n"
      ],
      "metadata": {
        "id": "q_HlZZPiyeNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly, all models are not able to classify this image correctly.\n",
        "## The End"
      ],
      "metadata": {
        "id": "7AWvag7GxuK9"
      }
    }
  ]
}