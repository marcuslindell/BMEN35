{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35/blob/main/Session3/BMEN35_Ex10_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a46184ae",
      "metadata": {
        "id": "a46184ae"
      },
      "source": [
        "# Assignment 3\n",
        "## Fill in your name below\n",
        "Alexander Andersson, BME4\n",
        "\n",
        "## Your mission is now the following:\n",
        "\n",
        "Part A) You will use sklearn to check if the Linear Regression model in the previous notebook is the same as the one in sklearn.\n",
        "\n",
        "Part B) You will use sklearn to check if the Logistic Regression model in the previous note is the same as the one ine sklearn.\n",
        "\n",
        "## When you have finished this assignment , save this notebook and submit it as assignment 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce3c800",
      "metadata": {
        "id": "9ce3c800"
      },
      "outputs": [],
      "source": [
        "# Part A\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lets start by making a simple example\n",
        "N = 500 # Number of datapoints\n",
        "X = 5 *np.random.rand(N,1)\n",
        "y =  2 - 3 * X + np.random.randn(N,1)\n",
        "\n",
        "# Lets make a scatter plot of the data to see what it looks like\n",
        "plt.scatter(X,y)\n",
        "plt.show()\n",
        "\n",
        "# We add a column of ones for the intercept (theta0)\n",
        "X_b = np.hstack((np.ones([N,1]),X))\n",
        "theta = np.dot(np.dot(np.linalg.inv(np.dot(X_b.T,X_b)),X_b.T),y)\n",
        "print(theta)\n",
        "\n",
        "# Here is where your code starts. We will start you off with\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = XXXX # Start filling in some code\n",
        "\n",
        "# The reg object will have some parameters named .intercept_ and .coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36392de",
      "metadata": {
        "id": "c36392de"
      },
      "outputs": [],
      "source": [
        "# Part B\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Four features x1,x2,x3,x4 and two classes (binary classification)\n",
        "X, y = make_classification(n_samples = 1000,n_features=4, n_classes = 2,random_state=0)\n",
        "# Lets split the data into a train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "X_train_i = np.c_[np.ones((np.shape(X_train)[0],1)),X_train] # Add 1 for intercept theta_0\n",
        "X_test_i = np.c_[np.ones((np.shape(X_test)[0],1)),X_test] #Add 1 for intercept theta_0\n",
        "theta = np.zeros((np.shape(X_train_i)[1])) # Intialize theta to zero\n",
        "# Lets again use gradient descent to find the right values for theta\n",
        "n_epochs = 20 # The number of epoch \n",
        "eta = 0.01 # Our learning rate\n",
        "J = np.zeros(n_epochs)\n",
        "for i in range(n_epochs):\n",
        "    theta_T_X = np.dot(X_train_i, theta)  # X*theta\n",
        "    #p_hat = 1 / (1 + np.exp(-theta_T_X))  # We pass this through the logistic function\n",
        "    p_hat = np.exp(theta_T_X) / (1 + np.exp(theta_T_X))\n",
        "    error = y_train - p_hat\n",
        "    J[i] = np.sum(-(y_train*np.log(p_hat) + (1-y_train)*np.log(1-p_hat)))/len(y_train) \n",
        "    grad = np.dot(X_train_i.T, error)\n",
        "    theta = theta + eta * grad\n",
        "    \n",
        "print(theta)\n",
        "y_hat = 1 / (1 + np.exp(-np.dot(X_test_i, theta))) # Calculate probabilities using our thetas.\n",
        "y_hat = np.round(y_hat) # Easy cheat instead of using if statements or similar\n",
        "acc = np.sum(y_hat == y_test)/len(y_test) # Calculate accuracy\n",
        "print(acc)\n",
        "\n",
        "# Here is where your code start. We will start you off with\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#clf = XXXX # Start filling in some code\n",
        "# The reg object will have some parameters named .intercept_ and .coef_ print them \n",
        "# Also predict based on Xtest and calculate accuracy to compare with the above code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a970c1",
      "metadata": {
        "id": "72a970c1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b87252d",
      "metadata": {
        "id": "6b87252d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}