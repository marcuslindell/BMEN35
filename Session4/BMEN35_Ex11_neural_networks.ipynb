{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmDF6SuasS6+pZ1cx9cwW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35/blob/main/Session4/BMEN35_Ex11_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural networks\n",
        "In this notebook we will have a look at neural network. The \"old school\" kind, shallow networks.\n",
        "\n",
        "As usual, we will start by import some libraries / modules, generating some data and visualising it."
      ],
      "metadata": {
        "id": "uxN0VBlzuaWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X,y = make_moons(n_samples = 300, noise=0.3, random_state=0)\n",
        "c = np.array(['r','b'])\n",
        "plt.scatter(X[:, 0], X[:, 1], c=c[y])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VyEFI9Biuu06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have two classes and two features and from the scatter-plot and it  looks like they are not linearly separable.\n",
        "\n",
        "What we can do is to use a neural network. We will use 1 hidden layer.\n",
        "\n",
        "Lets start with some bokkkeeping."
      ],
      "metadata": {
        "id": "bAHaXHjUu-kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X.shape[1] # Input dimensions, number of \"features\", 2 in this case\n",
        "num_classes = np.size(np.unique(y)) # Number of classes (2), binary in this case\n",
        "hidden_layer_size = 64 # no of neurons in hidden layer\n",
        "# initialize parameters \n",
        "W1 = 0.01 * np.random.randn(input_dim,hidden_layer_size) #  Weigths for input layer\n",
        "b1 = np.zeros((1,hidden_layer_size))             #  biases for input layer  \n",
        "W2 = 0.01 * np.random.randn(hidden_layer_size,num_classes) #  Weigths for hidden layer\n",
        "b2 = np.zeros((1,num_classes))             #  biases for hidden layer   \n",
        "\n",
        "eta = 0.1 # Learning rate\n",
        "iterations = 25000 # How many times we will run our loop, same as epochs\n",
        "num_examples = X.shape[0]\n",
        "loss = np.zeros(iterations)"
      ],
      "metadata": {
        "id": "217bi0GBwm25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have finished with our assigning of parameters and set other things. Let start with our algorithm."
      ],
      "metadata": {
        "id": "pSscn6v9yAnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(iterations):\n",
        "  L0 = X # Input layer\n",
        "  L1_in = np.dot(L0, W1) + b1 # Hidden layer input\n",
        "  L1_out = np.maximum(0, L1_in) # Hidden layer output, we use ReLU activation function here\n",
        "  L2_in = np.dot(L1_out, W2) + b2 # Output layer input\n",
        "  L2_out = np.exp(L2_in)/ np.sum(np.exp(L2_in), axis=1, keepdims=True) # Output layer output, there is also a softmax function in scipy.special\n",
        "  \n",
        "  cost =  -np.log(L2_out[range(num_examples),y])\n",
        "  loss[i] = np.sum(cost)/num_examples\n",
        "\n",
        "  # compute the gradient on L2out (6.26b in the book)\n",
        "  dL2out = L2_out\n",
        "  dL2out[range(num_examples),y] -= 1\n",
        "  dL2out = dL2out / num_examples\n",
        "\n",
        "  # backpropate the gradient to the parameters\n",
        "  # first backprop into parameters W2 and b2\n",
        "  dW2 = np.dot(L1_out.T, dL2out)\n",
        "  db2 = np.sum(dL2out, axis=0, keepdims=True)\n",
        "  # next backprop into hidden layer\n",
        "  dL1 = np.dot(L2_out, W2.T)\n",
        "  # backprop through ReLU non-linearity\n",
        "  dL1[L1_out <= 0] = 0\n",
        "  # finally into W,b\n",
        "  dW1 = np.dot(X.T, dL1)\n",
        "  db1 = np.sum(dL1, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "  # update weights and biases\n",
        "  W1 = W1 -eta * dW1\n",
        "  b1 = b1 -eta * db1\n",
        "  W2 = W2 -eta * dW2\n",
        "  b2 = b2 -eta * db2"
      ],
      "metadata": {
        "id": "YiOX6NqEyWPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have finished, lets check the loss."
      ],
      "metadata": {
        "id": "PVbw_8JBWEt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "qF0Nsc05y-cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, looks ok. The loss seems to have decreased. Lets check what we have for training accuracy. We will do a forward pass of the whole training dataset."
      ],
      "metadata": {
        "id": "jhwv3CAj4kJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L1_out = np.maximum(0, np.dot(X, W1) + b1)\n",
        "L2_out = np.dot(L1_out, W2) + b2\n",
        "y_hat = np.argmax(L2_out, axis=1)\n",
        "accuracy = np.sum(y_hat==y)/y.size # Calculate the classification accuracy\n",
        "print(accuracy)\n"
      ],
      "metadata": {
        "id": "SMoTM8T14yJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We seem to go quite good accuracy even though the data was not linearly separable. That is it for the introduction to neural networks tutorial.\n",
        "\n",
        "##The end"
      ],
      "metadata": {
        "id": "nlp1BG8oXR1R"
      }
    }
  ]
}